{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cdda9e8-d597-4894-a346-d8e64a9d9862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Reading text from pg5827.txt file\n",
    "with open('pg5827.txt', 'r') as textFile:\n",
    "    text = textFile.read()\n",
    "\n",
    "# Remove punctuation from the text, excluding apostrophes within words\n",
    "text = re.sub(r'\\b[.,;!\"“”()\\[\\]{}?<>]+\\B|\\B[.,;!\"“”()\\[\\]{}?<>]+\\b', '', text)\n",
    "\n",
    "# Converting to lowercase and tokenize by whitespace\n",
    "tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n",
    "\n",
    "# Count the frequency of each token\n",
    "word_count = Counter(tokens)\n",
    "\n",
    "# Sorting words alphabetically\n",
    "sorted_word_count = sorted(word_count.items())\n",
    "\n",
    "# Writing to CSV\n",
    "with open('all_words.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Word', 'Frequency'])\n",
    "    writer.writerows(sorted_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6608ad90-2707-4fa0-9a88-a12a69e9741b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Reading text from pg5827.txt file\n",
    "with open('pg5827.txt', 'r') as textFile:\n",
    "    text = textFile.read()\n",
    "\n",
    "# Extract all capitalized words\n",
    "capitalized_tokens = re.findall(r'\\b[A-Z][\\w\\']*[\\w\\']*\\b', text)\n",
    "\n",
    "# Count the frequency of each capitalized word\n",
    "capitalized_word_count = Counter(capitalized_tokens)\n",
    "\n",
    "# Sort capitalized words alphabetically\n",
    "sorted_capitalized_word_count = sorted(capitalized_word_count.items())\n",
    "\n",
    "# Write to CSV\n",
    "with open('all_uppercase_words.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write rows to CSV\n",
    "    for word, count in sorted_capitalized_word_count:\n",
    "        writer.writerow([word, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "716bddbd-0a9d-40f9-bc7a-6a90a8de06c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/salma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# First, make sure to download the NLTK stopwords if not done already.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Read the words and their frequencies from output.csv\n",
    "words_with_frequencies = {}\n",
    "with open('all_words.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)  # skip the headers\n",
    "    for row in reader:\n",
    "        word, frequency = row\n",
    "        words_with_frequencies[word] = int(frequency)\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from the words read from output.csv\n",
    "non_stopword_words_with_frequencies = {\n",
    "    word: freq for word, freq in words_with_frequencies.items() if word not in stop_words\n",
    "}\n",
    "\n",
    "# Sort non-stopword words alphabetically\n",
    "sorted_non_stopword_words_with_frequencies = dict(sorted(non_stopword_words_with_frequencies.items()))\n",
    "\n",
    "# Write the remaining non-stopword words and their frequencies to all_ns_words.csv\n",
    "with open('all_ns_words.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Word', 'Frequency'])  # Writing header\n",
    "    for word, frequency in sorted_non_stopword_words_with_frequencies.items():\n",
    "        writer.writerow([word, frequency])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b6b7a6c-9bfd-418f-b080-17d667da22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/salma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# First, make sure to download the NLTK stopwords if not done already.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Read words and frequencies from all_ns_words.csv\n",
    "words_with_frequencies = {}\n",
    "with open('all_ns_words.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)  # skip the headers\n",
    "    for row in reader:\n",
    "        word, frequency = row[0], int(row[1])\n",
    "        words_with_frequencies[word] = frequency\n",
    "\n",
    "# Calculate sum of frequencies of non-stopwords\n",
    "sum_non_stopwords = sum(words_with_frequencies.values())\n",
    "\n",
    "# Calculate probabilities for each word\n",
    "word_probabilities = {word: freq / sum_non_stopwords for word, freq in words_with_frequencies.items()}\n",
    "\n",
    "# Write the words, frequencies, and probabilities to the new CSV file\n",
    "with open('all_ns_words_with_probabilities.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Word', 'Frequency', 'Probability'])  # Writing header\n",
    "    for word, frequency in words_with_frequencies.items():\n",
    "        probability = word_probabilities[word]\n",
    "        writer.writerow([word, frequency, \"{:.5f}\".format(probability)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f182f3b2-47f2-422a-abfe-a8f4e46e42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/salma/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of probabilities for the first sentence is: 0.012740000000000001\n",
      "The sum of probabilities for the second sentence is: 0.01532\n",
      "The second sentence is more likely.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/salma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download NLTK data if not done already.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load probabilities from the file\n",
    "word_probabilities = {}\n",
    "with open('all_ns_words_with_probabilities.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)  # skip the headers\n",
    "    for row in reader:\n",
    "        word, probability = row[0], float(row[2])\n",
    "        word_probabilities[word] = probability\n",
    "\n",
    "# Define sentences\n",
    "sentence1 = \"If a belief is true, it can be deduced it is universal.\"\n",
    "sentence2 = \"Criticism of knowledge is counter to scientific results.\"\n",
    "\n",
    "# Tokenize the sentences and remove stopwords\n",
    "words1 = [word.lower() for word in word_tokenize(sentence1) if word.isalpha() and word.lower() not in stop_words]\n",
    "words2 = [word.lower() for word in word_tokenize(sentence2) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# Compute the sum of probabilities for each sentence\n",
    "sum_prob1 = sum(word_probabilities.get(word, 0) for word in words1)\n",
    "sum_prob2 = sum(word_probabilities.get(word, 0) for word in words2)\n",
    "\n",
    "# Print results\n",
    "print(\"The sum of probabilities for the first sentence is:\", sum_prob1)\n",
    "print(\"The sum of probabilities for the second sentence is:\", sum_prob2)\n",
    "\n",
    "if sum_prob1 > sum_prob2:\n",
    "    print(\"The first sentence is more likely.\")\n",
    "else:\n",
    "    print(\"The second sentence is more likely.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd93bd1-dbdb-4922-b96d-7304f9b8ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
